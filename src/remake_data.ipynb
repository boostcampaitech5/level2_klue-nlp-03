{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing data\n",
    "\n",
    "### train, val, test set 각각 cleaning data를 만듭니다\n",
    "1. data_cleaning.yaml파일을 다운로드 해주세요.\n",
    "1. pip install hanja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hanja\n",
    "import re\n",
    "import yaml\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.autonotebook import tqdm\n",
    "## data_cleaning.yaml 이랑 같은 폴더에 있어야합니다.\n",
    "with open(\"./data/data_cleaning.yaml\") as f:\n",
    "    cleaning = yaml.load(f, Loader=yaml.FullLoader)\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/remove_dup_train_split.csv')\n",
    "df = df.drop_duplicates(subset='sentence').reset_index(drop=True)\n",
    "df['subject_entity'] = df['subject_entity'].map(lambda x:eval(x))\n",
    "df['object_entity'] = df['object_entity'].map(lambda x:eval(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15701fe6a150406faed371b0712919a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, row in tqdm(df.iterrows()):\n",
    "    sent = ''\n",
    "    for char in row['sentence']:\n",
    "        try:\n",
    "            sent += cleaning[char]\n",
    "        except:\n",
    "            sent += char\n",
    "\n",
    "    sub= ''\n",
    "    for char in row['subject_entity']['word']:\n",
    "        try:\n",
    "            sub += cleaning[char]\n",
    "        except:\n",
    "            sub += char\n",
    "\n",
    "    obj = ''\n",
    "    for char in row['object_entity']['word']:\n",
    "        try:\n",
    "            obj += cleaning[char]\n",
    "        except:\n",
    "            obj += char\n",
    "    \n",
    "    df.loc[i, 'sentence'] = sent\n",
    "    df.loc[i, 'subject_entity']['word'] = sub\n",
    "    df.loc[i, 'object_entity']['word'] = obj\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4909f71438a9498d91dd8bcacaf89cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i , row in tqdm(df.iterrows()):\n",
    "    # chinese\n",
    "    if re.findall(r'[一-龥]+', row['sentence']):\n",
    "        df.loc[i, 'sentence'] = hanja.translate(row['sentence'], 'substitution')\n",
    "        df.loc[i, 'subject_entity']['word'] = hanja.translate(row['subject_entity']['word'], 'substitution')\n",
    "        df.loc[i, 'object_entity']['word'] =hanja.translate(row['object_entity']['word'], 'substitution')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19dda1375c045a2adac2b993fe7bb48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 불량 인덱스 검사 후 수정\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    sub_start = row['subject_entity']['start_idx']\n",
    "    sub_end = row['subject_entity']['end_idx']\n",
    "    obj_start = row['object_entity']['start_idx']\n",
    "    obj_end = row['object_entity']['end_idx']\n",
    "    sent = row['sentence']\n",
    "    sub = row['subject_entity']['word']\n",
    "    obj = row['object_entity']['word']\n",
    "    if sent[sub_start:sub_end+1] != sub or sent[obj_start:obj_end+1] != obj:\n",
    "        sub_start = sent.find(sub)\n",
    "        sub_end = sub_start + len(sub) - 1\n",
    "        obj_start = sent.find(obj)\n",
    "        obj_end = obj_start + len(obj) - 1\n",
    "        if sub_start < 0 or obj_start < 0:\n",
    "            print(sent)\n",
    "            print(sub, obj)\n",
    "            raise Exception(f\"{sub_start}, {obj_start}\")\n",
    "        print('fixed:',sub, sent[sub_start:sub_end+1])\n",
    "        print('fixed:',obj, sent[obj_start:obj_end+1])\n",
    "        df.loc[i,'subject_entity']['start_idx'] = sub_start\n",
    "        df.loc[i,'subject_entity']['end_idx'] = sub_end\n",
    "        df.loc[i,'object_entity']['start_idx'] = obj_start\n",
    "        df.loc[i,'object_entity']['end_idx'] = obj_end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09fe568f0ea45d2ae4dbacaddcdde67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: ['ь'] tokenized:  ['[UNK]']\n",
      "number of [UNK]: 1\n",
      "remain: ['/', 'ь', '·']\n"
     ]
    }
   ],
   "source": [
    "# 남은 토큰\n",
    "bad_word_unk = []\n",
    "bad_word_all = []\n",
    "pattern = r'[^ 0-9a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣\\.\\,\\?\\!\\:\\;\\'\\\"\\(\\)\\[\\]\\~\\-\\+\\_\\%\\<\\>\\《\\》@#$&\\*\\`\\{\\}\\=\\|]'\n",
    "for i , row in tqdm(df.iterrows()):\n",
    "    if re.findall(r'[一-龥]+|[ぁ-ゔ]+|[ァ-ヴー]+[々〆〤]+', row['sentence']):\n",
    "        print(row['sentence'])\n",
    "    bad = re.findall(pattern, df.loc[i, 'sentence'])\n",
    "    bad_word_all.extend(bad)\n",
    "    if bad:\n",
    "        tokenized = tokenizer.tokenize(''.join(bad), add_special_tokens=False)\n",
    "        if '[UNK]' in tokenized:\n",
    "            print('original:', bad, end=' ')\n",
    "            print('tokenized: ',tokenized)\n",
    "            bad_word_unk.extend(bad)\n",
    "        # print(df.loc[i, 'sentence'])\n",
    "bad_word_unk = list(set(bad_word_unk))\n",
    "bad_word_all = list(set(bad_word_all))\n",
    "print('number of [UNK]:', len(bad_word_unk))\n",
    "print('remain:',bad_word_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_cleaning.yaml에 추가하고 재시작해주세요.\n"
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "if bad_word_unk: print('data_cleaning.yaml에 추가하고 재시작해주세요.')\n",
    "for i in bad_word_unk:\n",
    "    tmp = unidecode(i)[0]\n",
    "    if re.findall(r'\\w',tmp):\n",
    "        if tmp == '@' or tmp=='#' or tmp=='^' or tmp=='*':\n",
    "            tmp = '-'\n",
    "        print(f'{i}: {tmp}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6347\n"
     ]
    }
   ],
   "source": [
    "df = df.sort_values('id').reset_index(drop=True)\n",
    "df.to_csv('./clean_train_data.csv', index=False)\n",
    "print(len(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
